{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oEVBWJgngJJR"
   },
   "source": [
    "#Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ycSY8L7YgaGw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXU6T_PWgrDo"
   },
   "source": [
    "#Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUid3gFLhkAA"
   },
   "source": [
    "###Traning Image preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m5vOzDMIgylU",
    "outputId": "7b155600-68b8-4f04-8346-e4ea1f3d8237"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3115 files belonging to 36 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = tf.keras.utils.image_dataset_from_directory(\n",
    "    'fruit-and-vegetable-image-recognition/train',\n",
    "    labels = 'inferred',\n",
    "    label_mode = 'categorical',\n",
    "    class_names = None,\n",
    "    color_mode = 'rgb',\n",
    "    batch_size = 32,\n",
    "    image_size = (64,64),\n",
    "    shuffle = True,\n",
    "    seed = None,\n",
    "    validation_split = None,\n",
    "    subset = None,\n",
    "    interpolation = 'bilinear',\n",
    "    follow_links = False,\n",
    "    crop_to_aspect_ratio = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lfZEKX6sXrL7"
   },
   "source": [
    "###validation image preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jdUrAU2IXvr1",
    "outputId": "0fb0cc28-b72f-494a-f619-b2f183d961a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 351 files belonging to 36 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_set = tf.keras.utils.image_dataset_from_directory(\n",
    "    'fruit-and-vegetable-image-recognition/validation',\n",
    "    labels = 'inferred',\n",
    "    label_mode = 'categorical',\n",
    "    class_names = None,\n",
    "    color_mode = 'rgb',\n",
    "    batch_size = 32,\n",
    "    image_size = (64,64),\n",
    "    shuffle = True,\n",
    "    seed = None,\n",
    "    validation_split = None,\n",
    "    subset = None,\n",
    "    interpolation = 'bilinear',\n",
    "    follow_links = False,\n",
    "    crop_to_aspect_ratio = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcVSoynXZYsZ"
   },
   "source": [
    "#Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ACaJsmPGd4SX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "98/98 [==============================] - 19s 153ms/step - loss: 7.1219 - accuracy: 0.0501 - val_loss: 3.3481 - val_accuracy: 0.1225\n",
      "Epoch 2/30\n",
      "98/98 [==============================] - 16s 152ms/step - loss: 5.1205 - accuracy: 0.0770 - val_loss: 7.4000 - val_accuracy: 0.0883\n",
      "Epoch 3/30\n",
      "98/98 [==============================] - 17s 157ms/step - loss: 3.8872 - accuracy: 0.1274 - val_loss: 2.8758 - val_accuracy: 0.2735\n",
      "Epoch 4/30\n",
      "98/98 [==============================] - 17s 157ms/step - loss: 3.9024 - accuracy: 0.1547 - val_loss: 2.5898 - val_accuracy: 0.4359\n",
      "Epoch 5/30\n",
      "98/98 [==============================] - 17s 160ms/step - loss: 3.5151 - accuracy: 0.1968 - val_loss: 2.2634 - val_accuracy: 0.4587\n",
      "Epoch 6/30\n",
      "98/98 [==============================] - 17s 158ms/step - loss: 3.2191 - accuracy: 0.2565 - val_loss: 1.9439 - val_accuracy: 0.5014\n",
      "Epoch 7/30\n",
      "98/98 [==============================] - 17s 157ms/step - loss: 2.6509 - accuracy: 0.3210 - val_loss: 2.5307 - val_accuracy: 0.3020\n",
      "Epoch 8/30\n",
      "98/98 [==============================] - 16s 156ms/step - loss: 2.3513 - accuracy: 0.3910 - val_loss: 1.4204 - val_accuracy: 0.6553\n",
      "Epoch 9/30\n",
      "98/98 [==============================] - 16s 156ms/step - loss: 2.2714 - accuracy: 0.4639 - val_loss: 1.4358 - val_accuracy: 0.6980\n",
      "Epoch 10/30\n",
      "98/98 [==============================] - 17s 159ms/step - loss: 1.9303 - accuracy: 0.5371 - val_loss: 1.4996 - val_accuracy: 0.7407\n",
      "Epoch 11/30\n",
      "98/98 [==============================] - 17s 158ms/step - loss: 1.9490 - accuracy: 0.5708 - val_loss: 1.2711 - val_accuracy: 0.7892\n",
      "Epoch 12/30\n",
      "98/98 [==============================] - 16s 157ms/step - loss: 1.6290 - accuracy: 0.6096 - val_loss: 1.2686 - val_accuracy: 0.8177\n",
      "Epoch 13/30\n",
      "98/98 [==============================] - 16s 157ms/step - loss: 1.6196 - accuracy: 0.6449 - val_loss: 1.0228 - val_accuracy: 0.8718\n",
      "Epoch 14/30\n",
      "98/98 [==============================] - 17s 157ms/step - loss: 1.4306 - accuracy: 0.6754 - val_loss: 1.0825 - val_accuracy: 0.8632\n",
      "Epoch 15/30\n",
      "98/98 [==============================] - 17s 163ms/step - loss: 1.5362 - accuracy: 0.6960 - val_loss: 1.1345 - val_accuracy: 0.8547\n",
      "Epoch 16/30\n",
      "98/98 [==============================] - 17s 158ms/step - loss: 1.2309 - accuracy: 0.7329 - val_loss: 1.2377 - val_accuracy: 0.8519\n",
      "Epoch 17/30\n",
      "98/98 [==============================] - 17s 163ms/step - loss: 1.2732 - accuracy: 0.7425 - val_loss: 1.5760 - val_accuracy: 0.7009\n",
      "Epoch 18/30\n",
      "98/98 [==============================] - 18s 169ms/step - loss: 1.0735 - accuracy: 0.7730 - val_loss: 1.4447 - val_accuracy: 0.8803\n",
      "Epoch 19/30\n",
      "98/98 [==============================] - 18s 169ms/step - loss: 1.2450 - accuracy: 0.7791 - val_loss: 1.1489 - val_accuracy: 0.9117\n",
      "Epoch 20/30\n",
      "98/98 [==============================] - 18s 167ms/step - loss: 1.2054 - accuracy: 0.7888 - val_loss: 1.1949 - val_accuracy: 0.9430\n",
      "Epoch 21/30\n",
      "98/98 [==============================] - 18s 168ms/step - loss: 0.8787 - accuracy: 0.8193 - val_loss: 2.2366 - val_accuracy: 0.7436\n",
      "Epoch 22/30\n",
      "98/98 [==============================] - 18s 169ms/step - loss: 0.9317 - accuracy: 0.8141 - val_loss: 1.1771 - val_accuracy: 0.9402\n",
      "Epoch 23/30\n",
      "98/98 [==============================] - 18s 169ms/step - loss: 1.2134 - accuracy: 0.8154 - val_loss: 1.0088 - val_accuracy: 0.8803\n",
      "Epoch 24/30\n",
      "98/98 [==============================] - 17s 166ms/step - loss: 0.9867 - accuracy: 0.8295 - val_loss: 1.8421 - val_accuracy: 0.8034\n",
      "Epoch 25/30\n",
      "98/98 [==============================] - 17s 160ms/step - loss: 0.9206 - accuracy: 0.8421 - val_loss: 1.3298 - val_accuracy: 0.9516\n",
      "Epoch 26/30\n",
      "98/98 [==============================] - 17s 159ms/step - loss: 0.8792 - accuracy: 0.8494 - val_loss: 0.8760 - val_accuracy: 0.9316\n",
      "Epoch 27/30\n",
      "98/98 [==============================] - 16s 157ms/step - loss: 0.7606 - accuracy: 0.8697 - val_loss: 0.9795 - val_accuracy: 0.9573\n",
      "Epoch 28/30\n",
      "98/98 [==============================] - 17s 158ms/step - loss: 0.8179 - accuracy: 0.8604 - val_loss: 0.9347 - val_accuracy: 0.9402\n",
      "Epoch 29/30\n",
      "98/98 [==============================] - 17s 158ms/step - loss: 1.0006 - accuracy: 0.8562 - val_loss: 1.1763 - val_accuracy: 0.9459\n",
      "Epoch 30/30\n",
      "98/98 [==============================] - 17s 158ms/step - loss: 0.6506 - accuracy: 0.8905 - val_loss: 1.0874 - val_accuracy: 0.9516\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "     cnn = tf.keras.models.Sequential()\n",
    "     ###Building Convolution Layer\n",
    "     cnn.add(tf.keras.layers.Conv2D(filters=64,kernel_size=3,activation='relu',input_shape=(64,64,3)))\n",
    "     cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))\n",
    "     cnn.add(tf.keras.layers.Conv2D(filters=64,kernel_size=3,activation='relu'))\n",
    "     cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))\n",
    "     cnn.add(tf.keras.layers.Dropout(0.5)) #To avoid overfitting\n",
    "     cnn.add(tf.keras.layers.Flatten())\n",
    "     cnn.add(tf.keras.layers.Dense(units=128,activation='relu'))\n",
    "     cnn.add(tf.keras.layers.Dense(units=36,activation='softmax'))\n",
    "     ##Compiling and Training Phase\n",
    "     cnn.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "     training_history = cnn.fit(x=training_set,validation_data=validation_set,epochs=30)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
